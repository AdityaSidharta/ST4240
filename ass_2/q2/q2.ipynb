{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import csv\n",
    "import pandas as pd\n",
    "import string\n",
    "from string import maketrans \n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas_summary import DataFrameSummary\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, LassoCV\n",
    "from keras.models import load_model\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, Activation, Dropout, BatchNormalization\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from keras import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('Train_rev1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[['FullDescription', 'SalaryNormalized']]\n",
    "df_train.columns = [['message', 'salary']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(s):\n",
    "    s = str(s)\n",
    "    s = s.translate(None, string.punctuation)\n",
    "    s = s.translate(None, '1234567890')\n",
    "    s = [word for word in s.split() if not (word.startswith('http'))]\n",
    "    s = ' '.join(s)\n",
    "    s = re.sub(' +',' ',s)\n",
    "    s = s.decode('ascii', 'ignore')\n",
    "    s = s.lower()\n",
    "    s = [word for word in s.split() if word not\\\n",
    "         in stopwords.words('english') + ['k']]\n",
    "    s = [wordnet_lemmatizer.lemmatize(word) for word in s]\n",
    "    sentence = ' '.join(s)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = df_train['message'].values\n",
    "n_message = len(messages)\n",
    "token_values = np.zeros(n_message).astype(str)\n",
    "for idx in tqdm(range(n_message)):\n",
    "    token_values[idx] = text_process(messages[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(token_values, 'token_values.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_values = joblib.load('token_values.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['engineering system analyst dorki',\n",
       "       'stress engineer glasgow salary c',\n",
       "       'mathematical modeller simulation', ...,\n",
       "       'position qualified teacher subje',\n",
       "       'position qualified teacher subje',\n",
       "       'entrepreneurial growing private '], dtype='|S32')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_full = df_train['salary'].values.astype(float).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "def mape_loss_func(labels, preds):\n",
    "    err = 100. * np.mean(np.abs(1. - np.true_divide(preds, labels))) \n",
    "    return err\n",
    "\n",
    "mape_loss  = make_scorer(mape_loss_func, \n",
    "                          greater_is_better=False)\n",
    "\n",
    "def mape_xgboost(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    err =  100. * np.mean(np.abs(1. - np.true_divide(preds, labels))) \n",
    "    return 'error', err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_train, idx_test = train_test_split(np.arange(len(Y_full)), test_size = 0.30, random_state = 1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train, Y_test = Y_full[idx_train], Y_full[idx_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49.08273120444054"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape_loss_func(Y_test, np.tile(Y_train.mean(), len(Y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = TfidfVectorizer()\n",
    "X_full = count_vectorizer.fit_transform(token_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X_full[idx_train], X_full[idx_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(244768, 31444)\n",
      "(244768,)\n"
     ]
    }
   ],
   "source": [
    "print X_full.shape\n",
    "print Y_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
       "   normalize=True, positive=False, precompute=False, random_state=None,\n",
       "   selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_count_model = Lasso(normalize = True)\n",
    "lasso_count_model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42.974332976841254"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape_loss_func(Y_test, lasso_count_model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           (None, 31444)             0         \n",
      "_________________________________________________________________\n",
      "BN0 (BatchNormalization)     (None, 31444)             125776    \n",
      "_________________________________________________________________\n",
      "Dense1 (Dense)               (None, 1000)              31445000  \n",
      "_________________________________________________________________\n",
      "BN1 (BatchNormalization)     (None, 1000)              4000      \n",
      "_________________________________________________________________\n",
      "Drop1 (Dropout)              (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "prediction (Dense)           (None, 1)                 1001      \n",
      "=================================================================\n",
      "Total params: 31,575,777\n",
      "Trainable params: 31,510,889\n",
      "Non-trainable params: 64,888\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape = (X_train.shape[1],), name='Input')\n",
    "X = BatchNormalization(name = 'BN0')(inputs)\n",
    "X = Dense(1000,  activation='relu', name='Dense1')(X)\n",
    "X = BatchNormalization(name = 'BN1')(X)\n",
    "X = Dropout(0.05, name = 'Drop1')(X)\n",
    "X = Dense(1, activation = 'relu', name = 'prediction')(X)\n",
    "\n",
    "tfidf_nn = Model(inputs, X, name='tfidf_nn')\n",
    "tfidf_nn.compile('adam', 'mean_absolute_error', [metrics.mae])\n",
    "tfidf_nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "171337/171337 [==============================] - 2816s 16ms/step - loss: 21582.0810 - mean_absolute_error: 21582.0810\n",
      "Epoch 2/3\n",
      "171337/171337 [==============================] - 2802s 16ms/step - loss: 13394.1716 - mean_absolute_error: 13394.1716\n",
      "Epoch 3/3\n",
      "171337/171337 [==============================] - 2789s 16ms/step - loss: 10150.4570 - mean_absolute_error: 10150.4570\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbca5ee8310>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_nn.fit(X_train, Y_train, batch_size= 32, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73431/73431 [==============================] - 90s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50.23116090942423"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape_loss_func(Y_test, tfidf_nn.predict(X_test, batch_size=32, verbose =1).reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_nn.save('tfidf_nn.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['doc_0', 'doc_1', 'doc_2', 'doc_3', 'doc_4']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_index = ['doc_' + str(x) for x in np.arange(len(token_values))]\n",
    "token_index[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabeledLineSentence(object):\n",
    "    def __init__(self, token_index, token_values):\n",
    "        self.token_index = token_index\n",
    "        self.token_values = token_values\n",
    "        self.n_token = len(token_values)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for idx in range(self.n_token):\n",
    "            yield gensim.models.doc2vec.LabeledSentence(words= self.token_values[idx].split(),tags=[self.token_index[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = LabeledLineSentence(token_index, token_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adityasidharta/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `LabeledSentence` (Class will be removed in 4.0.0, use TaggedDocument instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Doc2Vec(size=1000, window=10, min_count=5, workers=8, alpha=0.025, min_alpha=0.025) \n",
    "model.build_vocab(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/home/adityasidharta/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `LabeledSentence` (Class will be removed in 4.0.0, use TaggedDocument instead).\n",
      "  if __name__ == '__main__':\n",
      "100%|██████████| 10/10 [04:34<00:00, 27.44s/it]\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(10)):\n",
    "    model.train(gen, total_examples = model.corpus_count, epochs = 1)\n",
    "    model.alpha -= 0.002 \n",
    "    model.min_alpha = model.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adityasidharta/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('amazing', 0.7279897928237915),\n",
       " ('brilliant', 0.7134158611297607),\n",
       " ('superb', 0.7132153511047363),\n",
       " ('fantastic', 0.6837238073348999),\n",
       " ('fabulous', 0.6799948811531067),\n",
       " ('excellent', 0.6740776896476746),\n",
       " ('incredible', 0.6696920394897461),\n",
       " ('wonderful', 0.6357823014259338),\n",
       " ('exciting', 0.6275373697280884),\n",
       " ('unique', 0.6078590154647827)]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('great')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adityasidharta/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('modeller', 0.5226564407348633),\n",
       " ('draughtsman', 0.5153059363365173),\n",
       " ('cable', 0.5147504806518555),\n",
       " ('technician', 0.4939102828502655),\n",
       " ('installer', 0.49100038409233093),\n",
       " ('inspector', 0.490485817193985),\n",
       " ('fitter', 0.4816894233226776),\n",
       " ('engine', 0.4813685417175293),\n",
       " ('draughtsperson', 0.48029792308807373),\n",
       " ('layer', 0.47212496399879456)]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('engineer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full = np.zeros((len(Y_full), 1000))\n",
    "for idx in range(X_full.shape[0]):\n",
    "    X_full[idx, :] = model['doc_' + str(idx)].reshape(1, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X_full[idx_train], X_full[idx_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LassoCV(alphas=None, copy_X=True, cv=None, eps=0.001, fit_intercept=True,\n",
       "    max_iter=1000, n_alphas=100, n_jobs=1, normalize=True, positive=False,\n",
       "    precompute='auto', random_state=None, selection='cyclic', tol=0.0001,\n",
       "    verbose=False)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_count_model = LassoCV(normalize = True)\n",
    "lasso_count_model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43.65288372728921"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape_loss_func(Y_test, lasso_count_model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Input (InputLayer)           (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "BN0 (BatchNormalization)     (None, 1000)              4000      \n",
      "_________________________________________________________________\n",
      "Dense1 (Dense)               (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "BN1 (BatchNormalization)     (None, 1000)              4000      \n",
      "_________________________________________________________________\n",
      "Drop1 (Dropout)              (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "prediction (Dense)           (None, 1)                 1001      \n",
      "=================================================================\n",
      "Total params: 1,010,001\n",
      "Trainable params: 1,006,001\n",
      "Non-trainable params: 4,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape = (X_train.shape[1],), name='Input')\n",
    "X = BatchNormalization(name = 'BN0')(inputs)\n",
    "X = Dense(1000,  activation='relu', name='Dense1')(X)\n",
    "X = BatchNormalization(name = 'BN1')(X)\n",
    "X = Dropout(0.1, name = 'Drop1')(X)\n",
    "X = Dense(1000,  activation='relu', name='Dense2')(X)\n",
    "X = BatchNormalization(name = 'BN2')(X)\n",
    "X = Dense(1, activation = 'relu', name = 'prediction')(X)\n",
    "\n",
    "doc2vec_nn = Model(inputs, X, name='tfidf_nn')\n",
    "doc2vec_nn.compile('adam', 'mean_absolute_error', [metrics.mae])\n",
    "doc2vec_nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 154203 samples, validate on 17134 samples\n",
      "Epoch 1/3\n",
      "154203/154203 [==============================] - 144s 935us/step - loss: 16575.5271 - mean_absolute_error: 16575.5271 - val_loss: 11996.6073 - val_mean_absolute_error: 11996.6073\n",
      "Epoch 2/3\n",
      "154203/154203 [==============================] - 169s 1ms/step - loss: 11955.1949 - mean_absolute_error: 11955.1949 - val_loss: 11850.4487 - val_mean_absolute_error: 11850.4487\n",
      "Epoch 3/3\n",
      "154203/154203 [==============================] - 151s 982us/step - loss: 11878.8543 - mean_absolute_error: 11878.8543 - val_loss: 11826.2310 - val_mean_absolute_error: 11826.2310\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbc35bf9910>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2vec_nn.fit(X_train, Y_train, batch_size= 32, epochs=2, validation_split= 0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73431/73431 [==============================] - 11s 150us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "38.78692564241303"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape_loss_func(Y_test, doc2vec_nn.predict(X_test, batch_size=32, verbose =1).reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
